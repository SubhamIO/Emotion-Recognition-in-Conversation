{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SaarthiAI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvzb8KahyAtY",
        "colab_type": "text"
      },
      "source": [
        "# EMOTION PREDICTION IN CONVERSATIONS:\n",
        "\n",
        "## APPROACH :\n",
        "- We will create a very basic first model first and then improve it using different other features. We will also see how deep neural networks can be used and end this post with some ideas about ensembling in general.\n",
        "\n",
        "## This covers:\n",
        "- TFIDF\n",
        "- BOW\n",
        "- logistic regression\n",
        "- naive bayes\n",
        "- svm\n",
        "- xgboost\n",
        "- grid search\n",
        "- word vectors\n",
        "- LSTM\n",
        "- Customised Ensembling\n",
        "\n",
        "## Research PAPER: https://arxiv.org/pdf/1810.02508.pdf\n",
        "\n",
        "## DATASET : MELD : https://affective-meld.github.io/\n",
        "\n",
        "## Metric Used:  MULTI CLASS LOG LOSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr6AWCQ2m8TH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "176cdf71-d4ce-4f55-ec9a-574747419709"
      },
      "source": [
        "cd drive/My\\ Drive/MELD"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MELD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9avfQC3oyoRj",
        "colab_type": "text"
      },
      "source": [
        "## LOAD DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR8xssmrqE1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c4554d2f-9bd1-4474-ceaf-f43b8b01be2b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVC\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkNDkMK9yq01",
        "colab_type": "text"
      },
      "source": [
        "## LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhE5XmMQqJfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train_sent_emo.csv')\n",
        "cv = pd.read_csv('dev_sent_emo.csv')\n",
        "test = pd.read_csv('test_sent_emo.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrSWw7k1yuUr",
        "colab_type": "text"
      },
      "source": [
        "## EXPLORATORY DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPhcHd2JqYVX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b35f00d2-d6b7-49b2-bc4d-28f6285186e5"
      },
      "source": [
        "train.shape, cv.shape , test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9989, 11), (1109, 11), (2610, 11))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSkliokBquNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ce8d31a6-46b8-44a0-f6d2-1eeca7a6c486"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Sr No.', 'Utterance', 'Speaker', 'Emotion', 'Sentiment', 'Dialogue_ID',\n",
              "       'Utterance_ID', 'Season', 'Episode', 'StartTime', 'EndTime'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky0FRDvhroJi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "861527dd-f42b-45c3-eaf8-d254c2b892f2"
      },
      "source": [
        "cv.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Sr No.', 'Utterance', 'Speaker', 'Emotion', 'Sentiment', 'Dialogue_ID',\n",
              "       'Utterance_ID', 'Season', 'Episode', 'StartTime', 'EndTime'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MSey04Lrpty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "65a752aa-6941-48e8-f037-3c224fe0900a"
      },
      "source": [
        "test.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Sr No.', 'Utterance', 'Speaker', 'Emotion', 'Sentiment', 'Dialogue_ID',\n",
              "       'Utterance_ID', 'Season', 'Episode', 'StartTime', 'EndTime'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEEkh1bprrWx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "17713c35-597a-41ef-9886-2f0f46dfe3f2"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sr No.</th>\n",
              "      <th>Utterance</th>\n",
              "      <th>Speaker</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Dialogue_ID</th>\n",
              "      <th>Utterance_ID</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>StartTime</th>\n",
              "      <th>EndTime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>also I was the point person on my companys tr...</td>\n",
              "      <td>Chandler</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>00:16:16,059</td>\n",
              "      <td>00:16:21,731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>You mustve had your hands full.</td>\n",
              "      <td>The Interviewer</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>00:16:21,940</td>\n",
              "      <td>00:16:23,442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>That I did. That I did.</td>\n",
              "      <td>Chandler</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>00:16:23,442</td>\n",
              "      <td>00:16:26,389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>So lets talk a little bit about your duties.</td>\n",
              "      <td>The Interviewer</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>00:16:26,820</td>\n",
              "      <td>00:16:29,572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>My duties?  All right.</td>\n",
              "      <td>Chandler</td>\n",
              "      <td>surprise</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>00:16:34,452</td>\n",
              "      <td>00:16:40,917</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sr No.  ...       EndTime\n",
              "0       1  ...  00:16:21,731\n",
              "1       2  ...  00:16:23,442\n",
              "2       3  ...  00:16:26,389\n",
              "3       4  ...  00:16:29,572\n",
              "4       5  ...  00:16:40,917\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKcSVCMIr4g_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "fb0d8861-0991-41c8-a676-d90324b56afb"
      },
      "source": [
        "#Imbalanced Data\n",
        "ax = sns.countplot(x=train['Emotion'], data=train)\n",
        "print(train['Emotion'].value_counts())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neutral     4710\n",
            "joy         1743\n",
            "surprise    1205\n",
            "anger       1109\n",
            "sadness      683\n",
            "disgust      271\n",
            "fear         268\n",
            "Name: Emotion, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NctoU2quOfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "db9cb803-ec81-4c2e-b0d4-a6cff03931b9"
      },
      "source": [
        "#Imbalanced Data\n",
        "ax = sns.countplot(x=cv['Emotion'], data=cv)\n",
        "print(cv['Emotion'].value_counts())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neutral     470\n",
            "joy         163\n",
            "anger       153\n",
            "surprise    150\n",
            "sadness     111\n",
            "fear         40\n",
            "disgust      22\n",
            "Name: Emotion, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5lbjooKuOjM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "d9417c08-ce5b-403c-eb72-c0fe4b10d64f"
      },
      "source": [
        "#Imbalanced Data\n",
        "ax = sns.countplot(x=test['Emotion'], data=test)\n",
        "print(test['Emotion'].value_counts())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neutral     1256\n",
            "joy          402\n",
            "anger        345\n",
            "surprise     281\n",
            "sadness      208\n",
            "disgust       68\n",
            "fear          50\n",
            "Name: Emotion, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFU-IoS0uYgX",
        "colab_type": "text"
      },
      "source": [
        "### Observation : \n",
        "1. Neutral emotion is dominating in CV ,Test set and  Train set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9kL-2SZsMLn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "561ad19c-a791-439d-9c6c-d50a5acd8cf8"
      },
      "source": [
        "train.info() #No nulls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9989 entries, 0 to 9988\n",
            "Data columns (total 11 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Sr No.        9989 non-null   int64 \n",
            " 1   Utterance     9989 non-null   object\n",
            " 2   Speaker       9989 non-null   object\n",
            " 3   Emotion       9989 non-null   object\n",
            " 4   Sentiment     9989 non-null   object\n",
            " 5   Dialogue_ID   9989 non-null   int64 \n",
            " 6   Utterance_ID  9989 non-null   int64 \n",
            " 7   Season        9989 non-null   int64 \n",
            " 8   Episode       9989 non-null   int64 \n",
            " 9   StartTime     9989 non-null   object\n",
            " 10  EndTime       9989 non-null   object\n",
            "dtypes: int64(5), object(6)\n",
            "memory usage: 858.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2K9f1-Ls6HL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "3f9ed9a9-6cb4-4926-dbe3-6137ee0db915"
      },
      "source": [
        "cv.info()  #No nulls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1109 entries, 0 to 1108\n",
            "Data columns (total 11 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Sr No.        1109 non-null   int64 \n",
            " 1   Utterance     1109 non-null   object\n",
            " 2   Speaker       1109 non-null   object\n",
            " 3   Emotion       1109 non-null   object\n",
            " 4   Sentiment     1109 non-null   object\n",
            " 5   Dialogue_ID   1109 non-null   int64 \n",
            " 6   Utterance_ID  1109 non-null   int64 \n",
            " 7   Season        1109 non-null   int64 \n",
            " 8   Episode       1109 non-null   int64 \n",
            " 9   StartTime     1109 non-null   object\n",
            " 10  EndTime       1109 non-null   object\n",
            "dtypes: int64(5), object(6)\n",
            "memory usage: 95.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Zxf7h_s-md",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "f1124c10-f7c2-4e7b-e519-3fd9fcf45a08"
      },
      "source": [
        "test.info()  #No nulls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2610 entries, 0 to 2609\n",
            "Data columns (total 11 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Sr No.        2610 non-null   int64 \n",
            " 1   Utterance     2610 non-null   object\n",
            " 2   Speaker       2610 non-null   object\n",
            " 3   Emotion       2610 non-null   object\n",
            " 4   Sentiment     2610 non-null   object\n",
            " 5   Dialogue_ID   2610 non-null   int64 \n",
            " 6   Utterance_ID  2610 non-null   int64 \n",
            " 7   Season        2610 non-null   int64 \n",
            " 8   Episode       2610 non-null   int64 \n",
            " 9   StartTime     2610 non-null   object\n",
            " 10  EndTime       2610 non-null   object\n",
            "dtypes: int64(5), object(6)\n",
            "memory usage: 224.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um7uQHVnvVg1",
        "colab_type": "text"
      },
      "source": [
        "## Splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69sLvs-lvDjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train , y_train = train[['Utterance']] , train[['Emotion']]\n",
        "X_cv , y_cv = cv[['Utterance']] , cv[['Emotion']]\n",
        "X_test , y_test = test[['Utterance']] , test[['Emotion']]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvhtwpVXy1DT",
        "colab_type": "text"
      },
      "source": [
        "## Defining Multi Class LogLoss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3PWj4wgvkhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
        "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
        "    :param actual: Array containing the actual target classes\n",
        "    :param predicted: Matrix with class predictions, one probability per class\n",
        "    \"\"\"\n",
        "    # Convert 'actual' to a binary array if it's not already:\n",
        "    if len(actual.shape) == 1:\n",
        "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
        "        for i, val in enumerate(actual):\n",
        "            actual2[i, val] = 1\n",
        "        actual = actual2\n",
        "\n",
        "    clip = np.clip(predicted, eps, 1 - eps)\n",
        "    rows = actual.shape[0]\n",
        "    vsota = np.sum(actual * np.log(clip))\n",
        "    return -1.0 / rows * vsota"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEbiFXPFy6f6",
        "colab_type": "text"
      },
      "source": [
        "### Define a scorer to be used in Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2hnrIy-2KQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-3Gv2UTxdk4",
        "colab_type": "text"
      },
      "source": [
        "## Using LabelEncoder to vectorise labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRPTJO20vzar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y_train_enc = lbl_enc.fit_transform(y_train.Emotion.values)\n",
        "y_cv_enc = lbl_enc.transform(y_cv.Emotion.values)\n",
        "y_test_enc = lbl_enc.transform(y_test.Emotion.values)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toSv1Bfz-PS-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "07ee149e-f2c0-4100-f9eb-2431ae725e3f"
      },
      "source": [
        "y_train.Emotion.values[1:200]"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['neutral', 'neutral', 'neutral', 'surprise', 'neutral', 'neutral',\n",
              "       'neutral', 'neutral', 'neutral', 'fear', 'neutral', 'surprise',\n",
              "       'neutral', 'surprise', 'sadness', 'surprise', 'fear', 'neutral',\n",
              "       'neutral', 'neutral', 'neutral', 'neutral', 'joy', 'sadness',\n",
              "       'surprise', 'neutral', 'disgust', 'sadness', 'neutral', 'neutral',\n",
              "       'joy', 'neutral', 'joy', 'surprise', 'surprise', 'surprise',\n",
              "       'neutral', 'neutral', 'neutral', 'surprise', 'sadness', 'neutral',\n",
              "       'surprise', 'joy', 'surprise', 'neutral', 'neutral', 'neutral',\n",
              "       'neutral', 'neutral', 'joy', 'joy', 'joy', 'sadness', 'neutral',\n",
              "       'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'surprise',\n",
              "       'joy', 'surprise', 'joy', 'neutral', 'neutral', 'anger', 'joy',\n",
              "       'neutral', 'surprise', 'anger', 'anger', 'anger', 'neutral',\n",
              "       'neutral', 'sadness', 'sadness', 'sadness', 'surprise', 'anger',\n",
              "       'anger', 'anger', 'anger', 'neutral', 'anger', 'neutral',\n",
              "       'neutral', 'neutral', 'neutral', 'joy', 'neutral', 'joy',\n",
              "       'neutral', 'neutral', 'neutral', 'joy', 'neutral', 'neutral',\n",
              "       'neutral', 'neutral', 'neutral', 'joy', 'neutral', 'neutral',\n",
              "       'disgust', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger',\n",
              "       'neutral', 'neutral', 'anger', 'neutral', 'joy', 'neutral',\n",
              "       'neutral', 'joy', 'joy', 'joy', 'joy', 'neutral', 'joy', 'disgust',\n",
              "       'surprise', 'disgust', 'neutral', 'fear', 'neutral', 'surprise',\n",
              "       'fear', 'disgust', 'anger', 'joy', 'neutral', 'surprise',\n",
              "       'neutral', 'neutral', 'neutral', 'neutral', 'surprise', 'neutral',\n",
              "       'neutral', 'anger', 'neutral', 'neutral', 'sadness', 'surprise',\n",
              "       'sadness', 'anger', 'sadness', 'neutral', 'sadness', 'neutral',\n",
              "       'neutral', 'neutral', 'neutral', 'neutral', 'joy', 'anger',\n",
              "       'anger', 'anger', 'neutral', 'anger', 'joy', 'joy', 'joy', 'joy',\n",
              "       'disgust', 'surprise', 'neutral', 'neutral', 'anger', 'joy',\n",
              "       'neutral', 'neutral', 'fear', 'neutral', 'fear', 'joy', 'joy',\n",
              "       'joy', 'joy', 'neutral', 'neutral', 'neutral', 'joy', 'neutral',\n",
              "       'joy', 'fear', 'neutral', 'sadness', 'surprise', 'fear', 'neutral',\n",
              "       'neutral', 'neutral', 'joy'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMzIgekV-IKe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "55d8eb8d-bc4d-4dc4-e7cb-0837a82da45d"
      },
      "source": [
        "y_train_enc[1:200]"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 4, 4, 6, 4, 4, 4, 4, 4, 2, 4, 6, 4, 6, 5, 6, 2, 4, 4, 4, 4, 4,\n",
              "       3, 5, 6, 4, 1, 5, 4, 4, 3, 4, 3, 6, 6, 6, 4, 4, 4, 6, 5, 4, 6, 3,\n",
              "       6, 4, 4, 4, 4, 4, 3, 3, 3, 5, 4, 4, 4, 4, 4, 4, 6, 3, 6, 3, 4, 4,\n",
              "       0, 3, 4, 6, 0, 0, 0, 4, 4, 5, 5, 5, 6, 0, 0, 0, 0, 4, 0, 4, 4, 4,\n",
              "       4, 3, 4, 3, 4, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 1, 0, 0, 0, 0, 0,\n",
              "       0, 4, 4, 0, 4, 3, 4, 4, 3, 3, 3, 3, 4, 3, 1, 6, 1, 4, 2, 4, 6, 2,\n",
              "       1, 0, 3, 4, 6, 4, 4, 4, 4, 6, 4, 4, 0, 4, 4, 5, 6, 5, 0, 5, 4, 5,\n",
              "       4, 4, 4, 4, 4, 3, 0, 0, 0, 4, 0, 3, 3, 3, 3, 1, 6, 4, 4, 0, 3, 4,\n",
              "       4, 2, 4, 2, 3, 3, 3, 3, 4, 4, 4, 3, 4, 3, 2, 4, 5, 6, 2, 4, 4, 4,\n",
              "       3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YuZ1ZcEzaIT",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocesing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVppv_qhxQhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "### Dataset Preprocessing training set\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "train_corpus = []\n",
        "for i in range(0, len(X_train)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', X_train['Utterance'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    train_corpus.append(review)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8bW0T7GxVlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "### Dataset Preprocessing cv set\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "cv_corpus = []\n",
        "for i in range(0, len(X_cv)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', X_cv['Utterance'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    cv_corpus.append(review)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr3BDUG6yL8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "### Dataset Preprocessing test set\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "test_corpus = []\n",
        "for i in range(0, len(X_test)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', X_test['Utterance'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    test_corpus.append(review)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqZMbbdsy0Ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train['clean_utterance'] = train_corpus\n",
        "X_train.drop('Utterance',axis=1,inplace=True)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDDuaeLmzJ3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_cv['clean_utterance'] = cv_corpus\n",
        "X_cv.drop('Utterance',axis=1,inplace=True)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEmYnrqmzPih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test['clean_utterance'] = test_corpus\n",
        "X_test.drop('Utterance',axis=1,inplace=True)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj6KJRzkzKMp",
        "colab_type": "text"
      },
      "source": [
        "# MODELLING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIFLFPAUznGR",
        "colab_type": "text"
      },
      "source": [
        "## MODEL 1: TFIDF + LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89fw_nBvzjaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
        "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
        "            stop_words = 'english')\n",
        "\n",
        "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
        "tfv.fit(list(X_train['clean_utterance']) + list(X_cv['clean_utterance']) + list(X_test['clean_utterance']))\n",
        "X_train_tfv =  tfv.transform(X_train['clean_utterance']) \n",
        "X_valid_tfv = tfv.transform(X_cv['clean_utterance'])\n",
        "X_test_tfv = tfv.transform(X_test['clean_utterance'])"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XrrP_PQ0SYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f34db8e-7e03-4cfc-d997-6e1f659064a4"
      },
      "source": [
        "X_train_tfv.shape,X_valid_tfv.shape,X_test_tfv.shape"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9989, 3179), (1109, 3179), (2610, 3179))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lDgN8hn0WHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "b680f9a3-0192-44c8-d139-07d7a3f277ac"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "alpha = [10 ** x for x in range(-6, 3)]\n",
        "\n",
        "\n",
        "# initialize Our first RandomForestRegressor model...\n",
        "regr2 = LogisticRegression()\n",
        "\n",
        "# declare parameters for hyperparameter tuning\n",
        "parameters = {'C':alpha} \n",
        "\n",
        "# Perform cross validation \n",
        "clf = GridSearchCV(regr2,\n",
        "                    param_grid = parameters,\n",
        "                    scoring=mll_scorer,\n",
        "                    n_jobs = -1,\n",
        "                    verbose = 10, refit=True, cv=2)\n",
        "result = clf.fit(X_train_tfv, y_train_enc)\n",
        "\n",
        "# Summarize results\n",
        "print(\"Best: %f using %s\" % (result.best_score_, result.best_params_))\n",
        "means = result.cv_results_['mean_test_score']\n",
        "stds = result.cv_results_['std_test_score']\n",
        "params = result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f 1(%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    2.6s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.2s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    4.9s\n",
            "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    7.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: -1.417605 using {'C': 1}\n",
            "-1.536665 1(0.000177) with: {'C': 1e-06}\n",
            "-1.536648 1(0.000177) with: {'C': 1e-05}\n",
            "-1.536480 1(0.000177) with: {'C': 0.0001}\n",
            "-1.534834 1(0.000178) with: {'C': 0.001}\n",
            "-1.520896 1(0.000221) with: {'C': 0.01}\n",
            "-1.463340 1(0.000332) with: {'C': 0.1}\n",
            "-1.417605 1(0.004722) with: {'C': 1}\n",
            "-1.622374 1(0.001400) with: {'C': 10}\n",
            "-2.300202 1(0.032847) with: {'C': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAyCwQ9i8M03",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2ceb705e-8ca5-4783-e3ee-a6d293a829fe"
      },
      "source": [
        "lr = LogisticRegression(C = 1)\n",
        "lr.fit(X_train_tfv, y_train_enc)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoJ9Wtgz9AaV",
        "colab_type": "text"
      },
      "source": [
        "## Train,Test and CV loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPlep_T283zr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffeecc1b-4013-43cf-850c-64a1b301ad21"
      },
      "source": [
        "predictions = lr.predict_proba(X_train_tfv)\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_train_enc, predictions))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss: 1.129 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8lQedlk8M67",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1526aea-01e0-40fa-a7b4-8179d302ca06"
      },
      "source": [
        "predictions = lr.predict_proba(X_valid_tfv)\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_cv_enc, predictions))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss: 1.477 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHADdxNk8M5B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96c85955-8c47-4c8a-9754-5fe9c6b8835e"
      },
      "source": [
        "predictions = lr.predict_proba(X_test_tfv)\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_test_enc, predictions))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss: 1.372 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mqRQb37AwgZ",
        "colab_type": "text"
      },
      "source": [
        "## MODEL 2:BOW + LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgLLN2MR8Mys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), stop_words = 'english')\n",
        "\n",
        "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
        "ctv.fit(list(X_train['clean_utterance']) + list(X_cv['clean_utterance']) + list(X_test['clean_utterance']))\n",
        "X_train_ctv =  ctv.transform(X_train['clean_utterance']) \n",
        "X_valid_ctv = ctv.transform(X_cv['clean_utterance'])\n",
        "X_test_ctv = ctv.transform(X_test['clean_utterance'])"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWSdvqYO5eWd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7a491e5-38e6-41f8-c1d2-8ab06d032147"
      },
      "source": [
        "X_train_ctv.shape,X_valid_ctv.shape,X_test_ctv.shape"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9989, 51082), (1109, 51082), (2610, 51082))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCJ2kZV3CNi5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "66540e69-b82e-4a03-9a88-1661073171d9"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "alpha = [10 ** x for x in range(-6, 3)]\n",
        "\n",
        "\n",
        "# initialize Our first RandomForestRegressor model...\n",
        "regr2 = LogisticRegression()\n",
        "\n",
        "# declare parameters for hyperparameter tuning\n",
        "parameters = {'C':alpha} \n",
        "\n",
        "# Perform cross validation \n",
        "clf = GridSearchCV(regr2,\n",
        "                    param_grid = parameters,\n",
        "                    scoring=mll_scorer,\n",
        "                    n_jobs = -1,\n",
        "                    verbose = 10, refit=True, cv=2)\n",
        "result = clf.fit(X_train_ctv, y_train_enc)\n",
        "\n",
        "# Summarize results\n",
        "print(\"Best: %f using %s\" % (result.best_score_, result.best_params_))\n",
        "means = result.cv_results_['mean_test_score']\n",
        "stds = result.cv_results_['std_test_score']\n",
        "params = result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f 1(%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    6.5s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   12.7s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   15.9s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   29.6s\n",
            "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:   48.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: -1.427496 using {'C': 0.1}\n",
            "-1.536658 1(0.000176) with: {'C': 1e-06}\n",
            "-1.536569 1(0.000177) with: {'C': 1e-05}\n",
            "-1.535706 1(0.000178) with: {'C': 0.0001}\n",
            "-1.528054 1(0.000209) with: {'C': 0.001}\n",
            "-1.489068 1(0.000305) with: {'C': 0.01}\n",
            "-1.427496 1(0.001608) with: {'C': 0.1}\n",
            "-1.460494 1(0.008207) with: {'C': 1}\n",
            "-1.784036 1(0.024584) with: {'C': 10}\n",
            "-2.629078 1(0.109224) with: {'C': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtqmTS8ECQrE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5c66ce49-dc01-4ef0-ddb2-65e382bb04aa"
      },
      "source": [
        "lr = LogisticRegression(C = 0.1)\n",
        "lr.fit(X_train_ctv, y_train_enc)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raa5eHIjCXTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8cda0c9-9e8a-4db8-8dff-e5c34f6f865c"
      },
      "source": [
        "predictions = lr.predict_proba(X_train_ctv)\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_train_enc, predictions))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss: 1.115 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMA35ZTKCXWV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14babbd7-e272-46a3-8815-01f4898a54c6"
      },
      "source": [
        "predictions = lr.predict_proba(X_valid_ctv)\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_cv_enc, predictions))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss: 1.501 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ-bj0LdCXZ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f450d8d3-5726-4794-ff11-c095edfe5001"
      },
      "source": [
        "predictions = lr.predict_proba(X_test_ctv)\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_test_enc, predictions))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss: 1.394 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0_88yr3CqrF",
        "colab_type": "text"
      },
      "source": [
        "### since MODEL 1 was better so we will move forward with TFIDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UyuZ3WtdG2e",
        "colab_type": "text"
      },
      "source": [
        "## MODEL 3 : Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAq0S2TFI83Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n",
        "# make sure you have the glove_vectors file\n",
        "import pickle\n",
        "with open('glove_vectors', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "    glove_words =  set(model.keys())"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxg74P7iJsQX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bd03c2ed-7a9e-4654-f55f-525f95a225b7"
      },
      "source": [
        "# average Word2Vec\n",
        "# compute average word2vec for each review.\n",
        "avg_w2v_vectors_train = []; # the avg-w2v for each sentence/review is stored in this list\n",
        "for sentence in tqdm(X_train['clean_utterance']): # for each review/sentence\n",
        "    vector = np.zeros(300) # as word vectors are of zero length\n",
        "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sentence.split(): # for each word in a review/sentence\n",
        "        if word in glove_words:\n",
        "            vector += model[word]\n",
        "            cnt_words += 1\n",
        "    if cnt_words != 0:\n",
        "        vector /= cnt_words\n",
        "    avg_w2v_vectors_train.append(vector)\n",
        "\n",
        "print(len(avg_w2v_vectors_train))\n",
        "print(len(avg_w2v_vectors_train[0]))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9989/9989 [00:00<00:00, 70173.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9989\n",
            "300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCIuBrEGJsTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "96a143ab-d1d7-4eb8-ddb2-c948403b0ba8"
      },
      "source": [
        "# average Word2Vec\n",
        "# compute average word2vec for each review.\n",
        "avg_w2v_vectors_cv = []; # the avg-w2v for each sentence/review is stored in this list\n",
        "for sentence in tqdm(X_cv['clean_utterance']): # for each review/sentence\n",
        "    vector = np.zeros(300) # as word vectors are of zero length\n",
        "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sentence.split(): # for each word in a review/sentence\n",
        "        if word in glove_words:\n",
        "            vector += model[word]\n",
        "            cnt_words += 1\n",
        "    if cnt_words != 0:\n",
        "        vector /= cnt_words\n",
        "    avg_w2v_vectors_cv.append(vector)\n",
        "\n",
        "print(len(avg_w2v_vectors_cv))\n",
        "print(len(avg_w2v_vectors_cv[0]))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1109/1109 [00:00<00:00, 50739.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1109\n",
            "300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLwWcUfAJsOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cc7f1678-1848-4d4c-aa6d-308894530b9f"
      },
      "source": [
        "# average Word2Vec\n",
        "# compute average word2vec for each review.\n",
        "avg_w2v_vectors_test = []; # the avg-w2v for each sentence/review is stored in this list\n",
        "for sentence in tqdm(X_test['clean_utterance']): # for each review/sentence\n",
        "    vector = np.zeros(300) # as word vectors are of zero length\n",
        "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sentence.split(): # for each word in a review/sentence\n",
        "        if word in glove_words:\n",
        "            vector += model[word]\n",
        "            cnt_words += 1\n",
        "    if cnt_words != 0:\n",
        "        vector /= cnt_words\n",
        "    avg_w2v_vectors_test.append(vector)\n",
        "\n",
        "print(len(avg_w2v_vectors_test))\n",
        "print(len(avg_w2v_vectors_test[0]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2610/2610 [00:00<00:00, 59699.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2610\n",
            "300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0y5QgckKr0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain_glove = np.array(avg_w2v_vectors_train)\n",
        "xvalid_glove = np.array(avg_w2v_vectors_cv)\n",
        "xtest_glove = np.array(avg_w2v_vectors_test)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0-zJdGpVeTr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37cb5fb4-0695-449b-a372-8ec7b25e8fbb"
      },
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Fitting a simple xgboost on glove features\n",
        "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
        "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
        "clf.fit(xtrain_glove, y_train_enc)\n",
        "predictions = clf.predict_proba(xvalid_glove)\n",
        "\n",
        "\n",
        "print (\"Valid logloss: %0.3f \" % multiclass_logloss(y_cv_enc, predictions))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Valid logloss: 1.726 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roicf-rWZYO5",
        "colab_type": "text"
      },
      "source": [
        "## MODEL 4 : Truncated SVD + SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2RkUCc9ZJZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
        "svd = decomposition.TruncatedSVD(n_components=120)\n",
        "svd.fit(X_train_tfv)\n",
        "X_train_svd = svd.transform(X_train_tfv)\n",
        "X_valid_svd = svd.transform(X_valid_tfv)\n",
        "X_test_svd = svd.transform(X_test_tfv)\n",
        "\n",
        "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
        "scl = preprocessing.StandardScaler()\n",
        "scl.fit(X_train_svd)\n",
        "X_train_svd_scl = scl.transform(X_train_svd)\n",
        "X_valid_svd_scl = scl.transform(X_valid_svd)\n",
        "X_test_svd_scl = scl.transform(X_test_svd)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Wn5udAZbEM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f9fdaec-4e19-494c-da61-47afe0bade01"
      },
      "source": [
        "X_train_svd_scl.shape,X_valid_svd_scl.shape,X_test_svd_scl.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9989, 120), (1109, 120), (2610, 120))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIHlKzxdZrbY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "871dd795-a312-4624-c31f-7fe9a3d23feb"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# initialize Our first RandomForestRegressor model...\n",
        "svm = SVC(C=1)\n",
        "regr2 = CalibratedClassifierCV(svm)\n",
        "# declare parameters for hyperparameter tuning\n",
        "regr2.fit(X_train_svd_scl, y_train_enc)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CalibratedClassifierCV(base_estimator=SVC(C=1, break_ties=False, cache_size=200,\n",
              "                                          class_weight=None, coef0=0.0,\n",
              "                                          decision_function_shape='ovr',\n",
              "                                          degree=3, gamma='scale', kernel='rbf',\n",
              "                                          max_iter=-1, probability=False,\n",
              "                                          random_state=None, shrinking=True,\n",
              "                                          tol=0.001, verbose=False),\n",
              "                       cv=None, method='sigmoid')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaoEExoiaWuE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "976b4e51-18c6-42a5-bde4-b9232393aa2b"
      },
      "source": [
        "predictions = regr2.predict_proba(X_train_svd_scl)\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_train_enc, predictions))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss: 1.356 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Autsb_8Zxe7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "826bfca7-15bf-47af-d4c5-facedda5ab0a"
      },
      "source": [
        "predictions = regr2.predict_proba(X_valid_svd_scl)\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_cv_enc, predictions))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss: 1.556 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8fnSqtIaeQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6a0234d-1c2c-4c67-da2b-217cd7351b66"
      },
      "source": [
        "predictions = regr2.predict_proba(X_test_svd_scl)\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(y_test_enc, predictions))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss: 1.456 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDt5KXA2x16X",
        "colab_type": "text"
      },
      "source": [
        "# MODEL 5 : MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MVmozO_dPJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scale the data before any neural net:\n",
        "scl = preprocessing.StandardScaler()\n",
        "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
        "xvalid_glove_scl = scl.transform(xvalid_glove)\n",
        "xtest_glove_scl = scl.transform(xtest_glove)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtW2Bh_UanYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_enc_nn = np_utils.to_categorical(y_train_enc)\n",
        "y_cv_enc_nn = np_utils.to_categorical(y_cv_enc)\n",
        "y_test_enc_nn = np_utils.to_categorical(y_test_enc)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4SoYFS7dcVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(128, input_dim=300, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(7))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqe41vySdgDX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "28e017de-c1a3-491e-ec0a-96169a12a45c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_17 (Dense)             (None, 128)               38528     \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 7)                 903       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 174,215\n",
            "Trainable params: 172,679\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6f7x_SQeh1m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b8d20ff-6f5a-4543-a58c-c6ed3af7add5"
      },
      "source": [
        "y_cv_enc.shape"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1109,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsJz_n13dpwe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "0544ac94-3d98-471d-eeb3-ce3b4a1140e5"
      },
      "source": [
        "history = model.fit(xtrain_glove_scl, y=y_train_enc_nn, batch_size=64, \n",
        "          epochs=10, verbose=1, \n",
        "          validation_data=(xvalid_glove_scl, y_cv_enc_nn))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9989 samples, validate on 1109 samples\n",
            "Epoch 1/10\n",
            "9989/9989 [==============================] - 3s 262us/step - loss: 2.0361 - val_loss: 1.5816\n",
            "Epoch 2/10\n",
            "9989/9989 [==============================] - 2s 188us/step - loss: 1.6298 - val_loss: 1.5613\n",
            "Epoch 3/10\n",
            "9989/9989 [==============================] - 2s 186us/step - loss: 1.5424 - val_loss: 1.5323\n",
            "Epoch 4/10\n",
            "9989/9989 [==============================] - 2s 179us/step - loss: 1.4867 - val_loss: 1.5060\n",
            "Epoch 5/10\n",
            "9989/9989 [==============================] - 2s 181us/step - loss: 1.4608 - val_loss: 1.5041\n",
            "Epoch 6/10\n",
            "9989/9989 [==============================] - 2s 183us/step - loss: 1.4384 - val_loss: 1.4926\n",
            "Epoch 7/10\n",
            "9989/9989 [==============================] - 2s 180us/step - loss: 1.4248 - val_loss: 1.5016\n",
            "Epoch 8/10\n",
            "9989/9989 [==============================] - 2s 180us/step - loss: 1.4079 - val_loss: 1.4954\n",
            "Epoch 9/10\n",
            "9989/9989 [==============================] - 2s 182us/step - loss: 1.4059 - val_loss: 1.4948\n",
            "Epoch 10/10\n",
            "9989/9989 [==============================] - 2s 176us/step - loss: 1.3914 - val_loss: 1.4963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otBqIIeLeMFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using keras tokenizer here\n",
        "token = text.Tokenizer(num_words=None)\n",
        "max_len = 70\n",
        "a =list(X_train['clean_utterance']) + list(X_cv['clean_utterance']) + list(X_test['clean_utterance'])\n",
        "token.fit_on_texts(a)\n",
        "xtrain_seq = token.texts_to_sequences(X_train['clean_utterance'])\n",
        "xvalid_seq = token.texts_to_sequences(X_cv['clean_utterance'])\n",
        "xtest_seq = token.texts_to_sequences(X_test['clean_utterance'])\n",
        "\n",
        "# zero pad the sequences\n",
        "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
        "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
        "xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
        "\n",
        "word_index = token.word_index"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUoMC6B1fof-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "350217c9-1c47-41b1-a255-af6f6b23d6dc"
      },
      "source": [
        "# create an embedding matrix for the words we have in the dataset\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, i in tqdm(word_index.items()):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4709/4709 [00:00<00:00, 1526114.78it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNfK4oehhmWO",
        "colab_type": "text"
      },
      "source": [
        "# MODEL 6 : LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s5IoVT6gqwI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "799200a9-8992-4545-d574-961e2971c40e"
      },
      "source": [
        "# A simple LSTM with glove embeddings and two dense layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     weights=[embedding_matrix],\n",
        "                     input_length=max_len,\n",
        "                     trainable=False))\n",
        "model.add(SpatialDropout1D(0.3))\n",
        "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(7))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Fit the model with early stopping callback\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "history=model.fit(xtrain_pad, y=y_train_enc_nn, batch_size=512, epochs=200, verbose=1, validation_data=(xvalid_pad, y_cv_enc_nn), callbacks=[earlystop])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9989 samples, validate on 1109 samples\n",
            "Epoch 1/200\n",
            "9989/9989 [==============================] - 7s 658us/step - loss: 1.9394 - val_loss: 1.9347\n",
            "Epoch 2/200\n",
            "9989/9989 [==============================] - 6s 584us/step - loss: 1.9256 - val_loss: 1.9239\n",
            "Epoch 3/200\n",
            "9989/9989 [==============================] - 6s 593us/step - loss: 1.9122 - val_loss: 1.9134\n",
            "Epoch 4/200\n",
            "9989/9989 [==============================] - 6s 573us/step - loss: 1.8992 - val_loss: 1.9032\n",
            "Epoch 5/200\n",
            "9989/9989 [==============================] - 6s 587us/step - loss: 1.8867 - val_loss: 1.8934\n",
            "Epoch 6/200\n",
            "9989/9989 [==============================] - 6s 585us/step - loss: 1.8744 - val_loss: 1.8839\n",
            "Epoch 7/200\n",
            "9989/9989 [==============================] - 6s 591us/step - loss: 1.8627 - val_loss: 1.8746\n",
            "Epoch 8/200\n",
            "9989/9989 [==============================] - 6s 589us/step - loss: 1.8512 - val_loss: 1.8656\n",
            "Epoch 9/200\n",
            "9989/9989 [==============================] - 6s 593us/step - loss: 1.8400 - val_loss: 1.8569\n",
            "Epoch 10/200\n",
            "9989/9989 [==============================] - 6s 582us/step - loss: 1.8292 - val_loss: 1.8485\n",
            "Epoch 11/200\n",
            "9989/9989 [==============================] - 6s 586us/step - loss: 1.8188 - val_loss: 1.8402\n",
            "Epoch 12/200\n",
            "9989/9989 [==============================] - 6s 586us/step - loss: 1.8085 - val_loss: 1.8323\n",
            "Epoch 13/200\n",
            "9989/9989 [==============================] - 6s 590us/step - loss: 1.7987 - val_loss: 1.8245\n",
            "Epoch 14/200\n",
            "9989/9989 [==============================] - 6s 590us/step - loss: 1.7890 - val_loss: 1.8169\n",
            "Epoch 15/200\n",
            "9989/9989 [==============================] - 6s 577us/step - loss: 1.7797 - val_loss: 1.8095\n",
            "Epoch 16/200\n",
            "9989/9989 [==============================] - 6s 582us/step - loss: 1.7707 - val_loss: 1.8024\n",
            "Epoch 17/200\n",
            "9989/9989 [==============================] - 6s 584us/step - loss: 1.7618 - val_loss: 1.7954\n",
            "Epoch 18/200\n",
            "9989/9989 [==============================] - 6s 588us/step - loss: 1.7533 - val_loss: 1.7886\n",
            "Epoch 19/200\n",
            "9989/9989 [==============================] - 6s 587us/step - loss: 1.7450 - val_loss: 1.7820\n",
            "Epoch 20/200\n",
            "9989/9989 [==============================] - 6s 592us/step - loss: 1.7370 - val_loss: 1.7758\n",
            "Epoch 21/200\n",
            "9989/9989 [==============================] - 6s 581us/step - loss: 1.7292 - val_loss: 1.7697\n",
            "Epoch 22/200\n",
            "9989/9989 [==============================] - 6s 588us/step - loss: 1.7216 - val_loss: 1.7640\n",
            "Epoch 23/200\n",
            "9989/9989 [==============================] - 6s 570us/step - loss: 1.7144 - val_loss: 1.7582\n",
            "Epoch 24/200\n",
            "9989/9989 [==============================] - 6s 574us/step - loss: 1.7073 - val_loss: 1.7528\n",
            "Epoch 25/200\n",
            "9989/9989 [==============================] - 6s 579us/step - loss: 1.7005 - val_loss: 1.7476\n",
            "Epoch 26/200\n",
            "9989/9989 [==============================] - 6s 577us/step - loss: 1.6939 - val_loss: 1.7426\n",
            "Epoch 27/200\n",
            "9989/9989 [==============================] - 6s 563us/step - loss: 1.6875 - val_loss: 1.7376\n",
            "Epoch 28/200\n",
            "9989/9989 [==============================] - 6s 603us/step - loss: 1.6813 - val_loss: 1.7330\n",
            "Epoch 29/200\n",
            "9989/9989 [==============================] - 6s 585us/step - loss: 1.6755 - val_loss: 1.7285\n",
            "Epoch 30/200\n",
            "9989/9989 [==============================] - 6s 611us/step - loss: 1.6698 - val_loss: 1.7242\n",
            "Epoch 31/200\n",
            "9989/9989 [==============================] - 6s 583us/step - loss: 1.6643 - val_loss: 1.7202\n",
            "Epoch 32/200\n",
            "9989/9989 [==============================] - 6s 578us/step - loss: 1.6590 - val_loss: 1.7162\n",
            "Epoch 33/200\n",
            "9989/9989 [==============================] - 6s 571us/step - loss: 1.6539 - val_loss: 1.7124\n",
            "Epoch 34/200\n",
            "9989/9989 [==============================] - 6s 585us/step - loss: 1.6490 - val_loss: 1.7088\n",
            "Epoch 35/200\n",
            "9989/9989 [==============================] - 6s 598us/step - loss: 1.6443 - val_loss: 1.7053\n",
            "Epoch 36/200\n",
            "9989/9989 [==============================] - 6s 579us/step - loss: 1.6398 - val_loss: 1.7021\n",
            "Epoch 37/200\n",
            "9989/9989 [==============================] - 6s 572us/step - loss: 1.6355 - val_loss: 1.6989\n",
            "Epoch 38/200\n",
            "9989/9989 [==============================] - 6s 579us/step - loss: 1.6313 - val_loss: 1.6959\n",
            "Epoch 39/200\n",
            "9989/9989 [==============================] - 6s 568us/step - loss: 1.6273 - val_loss: 1.6931\n",
            "Epoch 40/200\n",
            "9989/9989 [==============================] - 6s 596us/step - loss: 1.6235 - val_loss: 1.6903\n",
            "Epoch 41/200\n",
            "9989/9989 [==============================] - 6s 585us/step - loss: 1.6198 - val_loss: 1.6878\n",
            "Epoch 42/200\n",
            "9989/9989 [==============================] - 6s 595us/step - loss: 1.6163 - val_loss: 1.6853\n",
            "Epoch 43/200\n",
            "9989/9989 [==============================] - 6s 577us/step - loss: 1.6129 - val_loss: 1.6829\n",
            "Epoch 44/200\n",
            "9989/9989 [==============================] - 6s 572us/step - loss: 1.6097 - val_loss: 1.6808\n",
            "Epoch 45/200\n",
            "9989/9989 [==============================] - 6s 587us/step - loss: 1.6065 - val_loss: 1.6786\n",
            "Epoch 46/200\n",
            "9989/9989 [==============================] - 6s 589us/step - loss: 1.6036 - val_loss: 1.6765\n",
            "Epoch 47/200\n",
            "9989/9989 [==============================] - 6s 588us/step - loss: 1.6007 - val_loss: 1.6746\n",
            "Epoch 48/200\n",
            "9989/9989 [==============================] - 6s 604us/step - loss: 1.5980 - val_loss: 1.6728\n",
            "Epoch 49/200\n",
            "9989/9989 [==============================] - 6s 586us/step - loss: 1.5954 - val_loss: 1.6710\n",
            "Epoch 50/200\n",
            "9989/9989 [==============================] - 6s 572us/step - loss: 1.5929 - val_loss: 1.6694\n",
            "Epoch 51/200\n",
            "9989/9989 [==============================] - 6s 585us/step - loss: 1.5904 - val_loss: 1.6678\n",
            "Epoch 52/200\n",
            "9989/9989 [==============================] - 6s 584us/step - loss: 1.5882 - val_loss: 1.6664\n",
            "Epoch 53/200\n",
            "9989/9989 [==============================] - 6s 596us/step - loss: 1.5860 - val_loss: 1.6648\n",
            "Epoch 54/200\n",
            "9989/9989 [==============================] - 6s 568us/step - loss: 1.5839 - val_loss: 1.6636\n",
            "Epoch 55/200\n",
            "9989/9989 [==============================] - 6s 596us/step - loss: 1.5819 - val_loss: 1.6624\n",
            "Epoch 56/200\n",
            "9989/9989 [==============================] - 6s 589us/step - loss: 1.5799 - val_loss: 1.6611\n",
            "Epoch 57/200\n",
            "9989/9989 [==============================] - 6s 588us/step - loss: 1.5781 - val_loss: 1.6599\n",
            "Epoch 58/200\n",
            "9989/9989 [==============================] - 6s 596us/step - loss: 1.5763 - val_loss: 1.6588\n",
            "Epoch 59/200\n",
            "9989/9989 [==============================] - 6s 579us/step - loss: 1.5746 - val_loss: 1.6579\n",
            "Epoch 60/200\n",
            "9989/9989 [==============================] - 6s 582us/step - loss: 1.5730 - val_loss: 1.6568\n",
            "Epoch 61/200\n",
            "9989/9989 [==============================] - 6s 584us/step - loss: 1.5715 - val_loss: 1.6558\n",
            "Epoch 62/200\n",
            "9989/9989 [==============================] - 6s 591us/step - loss: 1.5700 - val_loss: 1.6550\n",
            "Epoch 63/200\n",
            "9989/9989 [==============================] - 6s 584us/step - loss: 1.5686 - val_loss: 1.6540\n",
            "Epoch 64/200\n",
            "9989/9989 [==============================] - 6s 590us/step - loss: 1.5673 - val_loss: 1.6532\n",
            "Epoch 65/200\n",
            "9989/9989 [==============================] - 6s 635us/step - loss: 1.5660 - val_loss: 1.6525\n",
            "Epoch 66/200\n",
            "9989/9989 [==============================] - 6s 633us/step - loss: 1.5647 - val_loss: 1.6517\n",
            "Epoch 67/200\n",
            "9989/9989 [==============================] - 6s 613us/step - loss: 1.5636 - val_loss: 1.6511\n",
            "Epoch 68/200\n",
            "9989/9989 [==============================] - 6s 608us/step - loss: 1.5624 - val_loss: 1.6504\n",
            "Epoch 69/200\n",
            "9989/9989 [==============================] - 6s 610us/step - loss: 1.5613 - val_loss: 1.6497\n",
            "Epoch 70/200\n",
            "9989/9989 [==============================] - 6s 631us/step - loss: 1.5603 - val_loss: 1.6492\n",
            "Epoch 71/200\n",
            "9989/9989 [==============================] - 6s 621us/step - loss: 1.5593 - val_loss: 1.6486\n",
            "Epoch 72/200\n",
            "9989/9989 [==============================] - 6s 641us/step - loss: 1.5584 - val_loss: 1.6480\n",
            "Epoch 73/200\n",
            "9989/9989 [==============================] - 6s 620us/step - loss: 1.5574 - val_loss: 1.6475\n",
            "Epoch 74/200\n",
            "9989/9989 [==============================] - 6s 624us/step - loss: 1.5566 - val_loss: 1.6470\n",
            "Epoch 75/200\n",
            "9989/9989 [==============================] - 6s 617us/step - loss: 1.5557 - val_loss: 1.6466\n",
            "Epoch 76/200\n",
            "9989/9989 [==============================] - 6s 629us/step - loss: 1.5550 - val_loss: 1.6461\n",
            "Epoch 77/200\n",
            "9989/9989 [==============================] - 6s 609us/step - loss: 1.5542 - val_loss: 1.6458\n",
            "Epoch 78/200\n",
            "9989/9989 [==============================] - 6s 616us/step - loss: 1.5534 - val_loss: 1.6452\n",
            "Epoch 79/200\n",
            "9989/9989 [==============================] - 6s 603us/step - loss: 1.5527 - val_loss: 1.6448\n",
            "Epoch 80/200\n",
            "9989/9989 [==============================] - 6s 634us/step - loss: 1.5521 - val_loss: 1.6445\n",
            "Epoch 81/200\n",
            "9989/9989 [==============================] - 6s 612us/step - loss: 1.5514 - val_loss: 1.6441\n",
            "Epoch 82/200\n",
            "9989/9989 [==============================] - 6s 596us/step - loss: 1.5508 - val_loss: 1.6436\n",
            "Epoch 83/200\n",
            "9989/9989 [==============================] - 6s 596us/step - loss: 1.5502 - val_loss: 1.6433\n",
            "Epoch 84/200\n",
            "9989/9989 [==============================] - 6s 581us/step - loss: 1.5496 - val_loss: 1.6430\n",
            "Epoch 85/200\n",
            "9989/9989 [==============================] - 6s 575us/step - loss: 1.5491 - val_loss: 1.6427\n",
            "Epoch 86/200\n",
            "9989/9989 [==============================] - 6s 617us/step - loss: 1.5486 - val_loss: 1.6424\n",
            "Epoch 87/200\n",
            "9989/9989 [==============================] - 6s 619us/step - loss: 1.5481 - val_loss: 1.6422\n",
            "Epoch 88/200\n",
            "9989/9989 [==============================] - 6s 612us/step - loss: 1.5476 - val_loss: 1.6420\n",
            "Epoch 89/200\n",
            "9989/9989 [==============================] - 6s 605us/step - loss: 1.5471 - val_loss: 1.6418\n",
            "Epoch 90/200\n",
            "9989/9989 [==============================] - 6s 613us/step - loss: 1.5467 - val_loss: 1.6414\n",
            "Epoch 91/200\n",
            "9989/9989 [==============================] - 6s 594us/step - loss: 1.5463 - val_loss: 1.6412\n",
            "Epoch 92/200\n",
            "9989/9989 [==============================] - 6s 597us/step - loss: 1.5459 - val_loss: 1.6410\n",
            "Epoch 93/200\n",
            "9989/9989 [==============================] - 6s 576us/step - loss: 1.5455 - val_loss: 1.6407\n",
            "Epoch 94/200\n",
            "9989/9989 [==============================] - 6s 591us/step - loss: 1.5451 - val_loss: 1.6406\n",
            "Epoch 95/200\n",
            "9989/9989 [==============================] - 6s 582us/step - loss: 1.5447 - val_loss: 1.6403\n",
            "Epoch 96/200\n",
            "9989/9989 [==============================] - 6s 596us/step - loss: 1.5444 - val_loss: 1.6402\n",
            "Epoch 97/200\n",
            "9989/9989 [==============================] - 6s 584us/step - loss: 1.5441 - val_loss: 1.6399\n",
            "Epoch 98/200\n",
            "9989/9989 [==============================] - 6s 582us/step - loss: 1.5437 - val_loss: 1.6398\n",
            "Epoch 99/200\n",
            "9989/9989 [==============================] - 6s 573us/step - loss: 1.5434 - val_loss: 1.6397\n",
            "Epoch 100/200\n",
            "9989/9989 [==============================] - 6s 583us/step - loss: 1.5431 - val_loss: 1.6395\n",
            "Epoch 101/200\n",
            "9989/9989 [==============================] - 6s 585us/step - loss: 1.5429 - val_loss: 1.6394\n",
            "Epoch 102/200\n",
            "9989/9989 [==============================] - 6s 587us/step - loss: 1.5426 - val_loss: 1.6391\n",
            "Epoch 103/200\n",
            "9989/9989 [==============================] - 6s 579us/step - loss: 1.5423 - val_loss: 1.6389\n",
            "Epoch 104/200\n",
            "9989/9989 [==============================] - 6s 602us/step - loss: 1.5421 - val_loss: 1.6387\n",
            "Epoch 105/200\n",
            "9989/9989 [==============================] - 6s 602us/step - loss: 1.5418 - val_loss: 1.6386\n",
            "Epoch 106/200\n",
            "9989/9989 [==============================] - 6s 603us/step - loss: 1.5416 - val_loss: 1.6385\n",
            "Epoch 107/200\n",
            "9989/9989 [==============================] - 6s 608us/step - loss: 1.5414 - val_loss: 1.6383\n",
            "Epoch 108/200\n",
            "9989/9989 [==============================] - 6s 593us/step - loss: 1.5412 - val_loss: 1.6383\n",
            "Epoch 109/200\n",
            "9989/9989 [==============================] - 6s 587us/step - loss: 1.5410 - val_loss: 1.6382\n",
            "Epoch 110/200\n",
            "9989/9989 [==============================] - 6s 604us/step - loss: 1.5408 - val_loss: 1.6380\n",
            "Epoch 111/200\n",
            "9989/9989 [==============================] - 6s 578us/step - loss: 1.5406 - val_loss: 1.6379\n",
            "Epoch 112/200\n",
            "9989/9989 [==============================] - 6s 586us/step - loss: 1.5404 - val_loss: 1.6378\n",
            "Epoch 113/200\n",
            "9989/9989 [==============================] - 6s 584us/step - loss: 1.5403 - val_loss: 1.6377\n",
            "Epoch 114/200\n",
            "9989/9989 [==============================] - 6s 596us/step - loss: 1.5401 - val_loss: 1.6375\n",
            "Epoch 115/200\n",
            "9989/9989 [==============================] - 6s 595us/step - loss: 1.5399 - val_loss: 1.6374\n",
            "Epoch 116/200\n",
            "9989/9989 [==============================] - 6s 604us/step - loss: 1.5398 - val_loss: 1.6373\n",
            "Epoch 117/200\n",
            "9989/9989 [==============================] - 6s 579us/step - loss: 1.5397 - val_loss: 1.6373\n",
            "Epoch 118/200\n",
            "9989/9989 [==============================] - 6s 600us/step - loss: 1.5395 - val_loss: 1.6373\n",
            "Epoch 119/200\n",
            "9989/9989 [==============================] - 6s 582us/step - loss: 1.5394 - val_loss: 1.6371\n",
            "Epoch 120/200\n",
            "9989/9989 [==============================] - 6s 577us/step - loss: 1.5393 - val_loss: 1.6371\n",
            "Epoch 121/200\n",
            "9989/9989 [==============================] - 6s 592us/step - loss: 1.5392 - val_loss: 1.6370\n",
            "Epoch 122/200\n",
            "9989/9989 [==============================] - 6s 592us/step - loss: 1.5390 - val_loss: 1.6368\n",
            "Epoch 123/200\n",
            "9989/9989 [==============================] - 6s 594us/step - loss: 1.5389 - val_loss: 1.6367\n",
            "Epoch 124/200\n",
            "9989/9989 [==============================] - 6s 592us/step - loss: 1.5388 - val_loss: 1.6367\n",
            "Epoch 125/200\n",
            "9989/9989 [==============================] - 6s 596us/step - loss: 1.5387 - val_loss: 1.6367\n",
            "Epoch 126/200\n",
            "9989/9989 [==============================] - 6s 583us/step - loss: 1.5386 - val_loss: 1.6366\n",
            "Epoch 127/200\n",
            "9989/9989 [==============================] - 6s 584us/step - loss: 1.5385 - val_loss: 1.6365\n",
            "Epoch 128/200\n",
            "9989/9989 [==============================] - 6s 600us/step - loss: 1.5384 - val_loss: 1.6364\n",
            "Epoch 129/200\n",
            "9989/9989 [==============================] - 6s 601us/step - loss: 1.5383 - val_loss: 1.6365\n",
            "Epoch 130/200\n",
            "9989/9989 [==============================] - 6s 599us/step - loss: 1.5382 - val_loss: 1.6363\n",
            "Epoch 131/200\n",
            "9989/9989 [==============================] - 6s 591us/step - loss: 1.5382 - val_loss: 1.6364\n",
            "Epoch 132/200\n",
            "9989/9989 [==============================] - 6s 602us/step - loss: 1.5381 - val_loss: 1.6363\n",
            "Epoch 133/200\n",
            "9989/9989 [==============================] - 6s 595us/step - loss: 1.5380 - val_loss: 1.6361\n",
            "Epoch 134/200\n",
            "9989/9989 [==============================] - 6s 601us/step - loss: 1.5380 - val_loss: 1.6361\n",
            "Epoch 135/200\n",
            "9989/9989 [==============================] - 6s 599us/step - loss: 1.5379 - val_loss: 1.6361\n",
            "Epoch 136/200\n",
            "9989/9989 [==============================] - 6s 571us/step - loss: 1.5378 - val_loss: 1.6360\n",
            "Epoch 137/200\n",
            "9989/9989 [==============================] - 6s 595us/step - loss: 1.5378 - val_loss: 1.6360\n",
            "Epoch 138/200\n",
            "9989/9989 [==============================] - 6s 587us/step - loss: 1.5377 - val_loss: 1.6360\n",
            "Epoch 139/200\n",
            "9989/9989 [==============================] - 6s 595us/step - loss: 1.5377 - val_loss: 1.6359\n",
            "Epoch 140/200\n",
            "9989/9989 [==============================] - 6s 597us/step - loss: 1.5376 - val_loss: 1.6357\n",
            "Epoch 141/200\n",
            "9989/9989 [==============================] - 6s 584us/step - loss: 1.5376 - val_loss: 1.6357\n",
            "Epoch 142/200\n",
            "9989/9989 [==============================] - 6s 590us/step - loss: 1.5375 - val_loss: 1.6357\n",
            "Epoch 143/200\n",
            "9989/9989 [==============================] - 6s 597us/step - loss: 1.5375 - val_loss: 1.6358\n",
            "Epoch 144/200\n",
            "9989/9989 [==============================] - 6s 590us/step - loss: 1.5374 - val_loss: 1.6356\n",
            "Epoch 145/200\n",
            "9989/9989 [==============================] - 6s 581us/step - loss: 1.5374 - val_loss: 1.6357\n",
            "Epoch 146/200\n",
            "9989/9989 [==============================] - 6s 586us/step - loss: 1.5374 - val_loss: 1.6356\n",
            "Epoch 147/200\n",
            "9989/9989 [==============================] - 6s 585us/step - loss: 1.5373 - val_loss: 1.6355\n",
            "Epoch 148/200\n",
            "9989/9989 [==============================] - 6s 576us/step - loss: 1.5373 - val_loss: 1.6355\n",
            "Epoch 149/200\n",
            "9989/9989 [==============================] - 6s 578us/step - loss: 1.5373 - val_loss: 1.6355\n",
            "Epoch 150/200\n",
            "9989/9989 [==============================] - 6s 577us/step - loss: 1.5372 - val_loss: 1.6355\n",
            "Epoch 151/200\n",
            "9989/9989 [==============================] - 6s 599us/step - loss: 1.5372 - val_loss: 1.6354\n",
            "Epoch 152/200\n",
            "9989/9989 [==============================] - 6s 595us/step - loss: 1.5372 - val_loss: 1.6355\n",
            "Epoch 153/200\n",
            "9989/9989 [==============================] - 6s 588us/step - loss: 1.5371 - val_loss: 1.6354\n",
            "Epoch 154/200\n",
            "9989/9989 [==============================] - 6s 587us/step - loss: 1.5371 - val_loss: 1.6355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqikSJf1m2yW",
        "colab_type": "text"
      },
      "source": [
        "### We would obviouslt get better results with LSTM but we need more epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0COfXM9Sjiku",
        "colab_type": "text"
      },
      "source": [
        "# MODEL 7 : Bi -LSTM\n",
        "One question could be: why do i use so much dropout? Well, fit the model with no or little dropout and you will that it starts to overfit :)\n",
        "\n",
        "Let's see if Bi-directional LSTM can give us better results. Its a piece of cake to do it with Keras :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhBSQtbHiqVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b890b452-dff3-4c47-974f-b043f2ed2e4c"
      },
      "source": [
        "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                     300,\n",
        "                     weights=[embedding_matrix],\n",
        "                     input_length=max_len,\n",
        "                     trainable=False))\n",
        "model.add(SpatialDropout1D(0.3))\n",
        "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(7))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Fit the model with early stopping callback\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "history=model.fit(xtrain_pad, y=y_train_enc_nn, batch_size=512, epochs=200, verbose=1, validation_data=(xvalid_pad, y_cv_enc_nn), callbacks=[earlystop])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9989 samples, validate on 1109 samples\n",
            "Epoch 1/200\n",
            "9989/9989 [==============================] - 13s 1ms/step - loss: 1.9397 - val_loss: 1.9351\n",
            "Epoch 2/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.9259 - val_loss: 1.9240\n",
            "Epoch 3/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.9122 - val_loss: 1.9135\n",
            "Epoch 4/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.8991 - val_loss: 1.9033\n",
            "Epoch 5/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.8864 - val_loss: 1.8934\n",
            "Epoch 6/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.8742 - val_loss: 1.8838\n",
            "Epoch 7/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.8623 - val_loss: 1.8745\n",
            "Epoch 8/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.8508 - val_loss: 1.8655\n",
            "Epoch 9/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.8396 - val_loss: 1.8567\n",
            "Epoch 10/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.8288 - val_loss: 1.8482\n",
            "Epoch 11/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.8183 - val_loss: 1.8401\n",
            "Epoch 12/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.8081 - val_loss: 1.8320\n",
            "Epoch 13/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7982 - val_loss: 1.8242\n",
            "Epoch 14/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7886 - val_loss: 1.8166\n",
            "Epoch 15/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7792 - val_loss: 1.8092\n",
            "Epoch 16/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7701 - val_loss: 1.8021\n",
            "Epoch 17/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7613 - val_loss: 1.7951\n",
            "Epoch 18/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7528 - val_loss: 1.7885\n",
            "Epoch 19/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.7445 - val_loss: 1.7819\n",
            "Epoch 20/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.7365 - val_loss: 1.7757\n",
            "Epoch 21/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.7287 - val_loss: 1.7696\n",
            "Epoch 22/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7212 - val_loss: 1.7637\n",
            "Epoch 23/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7139 - val_loss: 1.7582\n",
            "Epoch 24/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7069 - val_loss: 1.7527\n",
            "Epoch 25/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.7001 - val_loss: 1.7475\n",
            "Epoch 26/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6936 - val_loss: 1.7425\n",
            "Epoch 27/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6873 - val_loss: 1.7376\n",
            "Epoch 28/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6811 - val_loss: 1.7329\n",
            "Epoch 29/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6752 - val_loss: 1.7285\n",
            "Epoch 30/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6695 - val_loss: 1.7243\n",
            "Epoch 31/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6641 - val_loss: 1.7201\n",
            "Epoch 32/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6588 - val_loss: 1.7161\n",
            "Epoch 33/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.6537 - val_loss: 1.7124\n",
            "Epoch 34/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6488 - val_loss: 1.7088\n",
            "Epoch 35/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6442 - val_loss: 1.7054\n",
            "Epoch 36/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6397 - val_loss: 1.7021\n",
            "Epoch 37/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6353 - val_loss: 1.6990\n",
            "Epoch 38/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6312 - val_loss: 1.6960\n",
            "Epoch 39/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6272 - val_loss: 1.6932\n",
            "Epoch 40/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6234 - val_loss: 1.6905\n",
            "Epoch 41/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6197 - val_loss: 1.6879\n",
            "Epoch 42/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.6162 - val_loss: 1.6854\n",
            "Epoch 43/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6128 - val_loss: 1.6831\n",
            "Epoch 44/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6096 - val_loss: 1.6808\n",
            "Epoch 45/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6065 - val_loss: 1.6786\n",
            "Epoch 46/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6035 - val_loss: 1.6767\n",
            "Epoch 47/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.6007 - val_loss: 1.6747\n",
            "Epoch 48/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5979 - val_loss: 1.6729\n",
            "Epoch 49/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5954 - val_loss: 1.6712\n",
            "Epoch 50/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5928 - val_loss: 1.6695\n",
            "Epoch 51/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5904 - val_loss: 1.6679\n",
            "Epoch 52/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5882 - val_loss: 1.6663\n",
            "Epoch 53/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5860 - val_loss: 1.6650\n",
            "Epoch 54/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5839 - val_loss: 1.6637\n",
            "Epoch 55/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5819 - val_loss: 1.6623\n",
            "Epoch 56/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5799 - val_loss: 1.6611\n",
            "Epoch 57/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5781 - val_loss: 1.6599\n",
            "Epoch 58/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5763 - val_loss: 1.6588\n",
            "Epoch 59/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5746 - val_loss: 1.6578\n",
            "Epoch 60/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5730 - val_loss: 1.6567\n",
            "Epoch 61/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5715 - val_loss: 1.6558\n",
            "Epoch 62/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5700 - val_loss: 1.6549\n",
            "Epoch 63/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5686 - val_loss: 1.6541\n",
            "Epoch 64/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5673 - val_loss: 1.6533\n",
            "Epoch 65/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5660 - val_loss: 1.6525\n",
            "Epoch 66/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5647 - val_loss: 1.6519\n",
            "Epoch 67/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5636 - val_loss: 1.6511\n",
            "Epoch 68/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5624 - val_loss: 1.6505\n",
            "Epoch 69/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5614 - val_loss: 1.6499\n",
            "Epoch 70/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5603 - val_loss: 1.6492\n",
            "Epoch 71/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5593 - val_loss: 1.6486\n",
            "Epoch 72/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5584 - val_loss: 1.6481\n",
            "Epoch 73/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5575 - val_loss: 1.6475\n",
            "Epoch 74/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5566 - val_loss: 1.6471\n",
            "Epoch 75/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5558 - val_loss: 1.6466\n",
            "Epoch 76/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5549 - val_loss: 1.6460\n",
            "Epoch 77/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5542 - val_loss: 1.6457\n",
            "Epoch 78/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5535 - val_loss: 1.6453\n",
            "Epoch 79/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5527 - val_loss: 1.6449\n",
            "Epoch 80/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5521 - val_loss: 1.6445\n",
            "Epoch 81/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5514 - val_loss: 1.6441\n",
            "Epoch 82/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5508 - val_loss: 1.6437\n",
            "Epoch 83/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5502 - val_loss: 1.6434\n",
            "Epoch 84/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5496 - val_loss: 1.6431\n",
            "Epoch 85/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5491 - val_loss: 1.6428\n",
            "Epoch 86/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5486 - val_loss: 1.6425\n",
            "Epoch 87/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5481 - val_loss: 1.6422\n",
            "Epoch 88/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5476 - val_loss: 1.6420\n",
            "Epoch 89/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5471 - val_loss: 1.6417\n",
            "Epoch 90/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5467 - val_loss: 1.6415\n",
            "Epoch 91/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5462 - val_loss: 1.6412\n",
            "Epoch 92/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5458 - val_loss: 1.6410\n",
            "Epoch 93/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5454 - val_loss: 1.6408\n",
            "Epoch 94/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5451 - val_loss: 1.6406\n",
            "Epoch 95/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5447 - val_loss: 1.6403\n",
            "Epoch 96/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5444 - val_loss: 1.6402\n",
            "Epoch 97/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5441 - val_loss: 1.6398\n",
            "Epoch 98/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5437 - val_loss: 1.6396\n",
            "Epoch 99/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5434 - val_loss: 1.6395\n",
            "Epoch 100/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5431 - val_loss: 1.6392\n",
            "Epoch 101/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5428 - val_loss: 1.6391\n",
            "Epoch 102/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5426 - val_loss: 1.6390\n",
            "Epoch 103/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5423 - val_loss: 1.6388\n",
            "Epoch 104/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5421 - val_loss: 1.6387\n",
            "Epoch 105/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5418 - val_loss: 1.6386\n",
            "Epoch 106/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5416 - val_loss: 1.6385\n",
            "Epoch 107/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5414 - val_loss: 1.6383\n",
            "Epoch 108/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5412 - val_loss: 1.6384\n",
            "Epoch 109/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5410 - val_loss: 1.6381\n",
            "Epoch 110/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5408 - val_loss: 1.6380\n",
            "Epoch 111/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5406 - val_loss: 1.6379\n",
            "Epoch 112/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5404 - val_loss: 1.6378\n",
            "Epoch 113/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5403 - val_loss: 1.6377\n",
            "Epoch 114/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5401 - val_loss: 1.6376\n",
            "Epoch 115/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5399 - val_loss: 1.6374\n",
            "Epoch 116/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5398 - val_loss: 1.6374\n",
            "Epoch 117/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5397 - val_loss: 1.6373\n",
            "Epoch 118/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5395 - val_loss: 1.6372\n",
            "Epoch 119/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5394 - val_loss: 1.6371\n",
            "Epoch 120/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5392 - val_loss: 1.6370\n",
            "Epoch 121/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5391 - val_loss: 1.6370\n",
            "Epoch 122/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5390 - val_loss: 1.6369\n",
            "Epoch 123/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5389 - val_loss: 1.6368\n",
            "Epoch 124/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5388 - val_loss: 1.6366\n",
            "Epoch 125/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5387 - val_loss: 1.6366\n",
            "Epoch 126/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5386 - val_loss: 1.6366\n",
            "Epoch 127/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5385 - val_loss: 1.6366\n",
            "Epoch 128/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5384 - val_loss: 1.6365\n",
            "Epoch 129/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5383 - val_loss: 1.6364\n",
            "Epoch 130/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5382 - val_loss: 1.6363\n",
            "Epoch 131/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5382 - val_loss: 1.6363\n",
            "Epoch 132/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5381 - val_loss: 1.6363\n",
            "Epoch 133/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5380 - val_loss: 1.6361\n",
            "Epoch 134/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5379 - val_loss: 1.6362\n",
            "Epoch 135/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5379 - val_loss: 1.6361\n",
            "Epoch 136/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5378 - val_loss: 1.6360\n",
            "Epoch 137/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5378 - val_loss: 1.6360\n",
            "Epoch 138/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5377 - val_loss: 1.6360\n",
            "Epoch 139/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5377 - val_loss: 1.6359\n",
            "Epoch 140/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5376 - val_loss: 1.6358\n",
            "Epoch 141/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5376 - val_loss: 1.6359\n",
            "Epoch 142/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5375 - val_loss: 1.6358\n",
            "Epoch 143/200\n",
            "9989/9989 [==============================] - 12s 1ms/step - loss: 1.5375 - val_loss: 1.6358\n",
            "Epoch 144/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5374 - val_loss: 1.6357\n",
            "Epoch 145/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5374 - val_loss: 1.6356\n",
            "Epoch 146/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5374 - val_loss: 1.6357\n",
            "Epoch 147/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5373 - val_loss: 1.6357\n",
            "Epoch 148/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5373 - val_loss: 1.6356\n",
            "Epoch 149/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5373 - val_loss: 1.6356\n",
            "Epoch 150/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5372 - val_loss: 1.6355\n",
            "Epoch 151/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5372 - val_loss: 1.6354\n",
            "Epoch 152/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5372 - val_loss: 1.6354\n",
            "Epoch 153/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5371 - val_loss: 1.6354\n",
            "Epoch 154/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5371 - val_loss: 1.6355\n",
            "Epoch 155/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5371 - val_loss: 1.6355\n",
            "Epoch 156/200\n",
            "9989/9989 [==============================] - 11s 1ms/step - loss: 1.5371 - val_loss: 1.6355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWCKLmfpZ7q",
        "colab_type": "text"
      },
      "source": [
        "## MODEL 8 : Trying customised Ensembling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9kjlouSpcaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is the main ensembling class. how to use it is in the next cell!\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.DEBUG,\n",
        "    format=\"[%(asctime)s] %(levelname)s %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\", stream=sys.stdout)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Ensembler(object):\n",
        "    def __init__(self, model_dict, num_folds=3, task_type='classification', optimize=roc_auc_score,\n",
        "                 lower_is_better=False, save_path=None):\n",
        "        \"\"\"\n",
        "        Ensembler init function\n",
        "        :param model_dict: model dictionary, see README for its format\n",
        "        :param num_folds: the number of folds for ensembling\n",
        "        :param task_type: classification or regression\n",
        "        :param optimize: the function to optimize for, e.g. AUC, logloss, etc. Must have two arguments y_test and y_pred\n",
        "        :param lower_is_better: is lower value of optimization function better or higher\n",
        "        :param save_path: path to which model pickles will be dumped to along with generated predictions, or None\n",
        "        \"\"\"\n",
        "\n",
        "        self.model_dict = model_dict\n",
        "        self.levels = len(self.model_dict)\n",
        "        self.num_folds = num_folds\n",
        "        self.task_type = task_type\n",
        "        self.optimize = optimize\n",
        "        self.lower_is_better = lower_is_better\n",
        "        self.save_path = save_path\n",
        "\n",
        "        self.training_data = None\n",
        "        self.test_data = None\n",
        "        self.y = None\n",
        "        self.lbl_enc = None\n",
        "        self.y_enc = None\n",
        "        self.train_prediction_dict = None\n",
        "        self.test_prediction_dict = None\n",
        "        self.num_classes = None\n",
        "\n",
        "    def fit(self, training_data, y, lentrain):\n",
        "        \"\"\"\n",
        "        :param training_data: training data in tabular format\n",
        "        :param y: binary, multi-class or regression\n",
        "        :return: chain of models to be used in prediction\n",
        "        \"\"\"\n",
        "\n",
        "        self.training_data = training_data\n",
        "        self.y = y\n",
        "\n",
        "        if self.task_type == 'classification':\n",
        "            self.num_classes = len(np.unique(self.y))\n",
        "            logger.info(\"Found %d classes\", self.num_classes)\n",
        "            self.lbl_enc = LabelEncoder()\n",
        "            self.y_enc = self.lbl_enc.fit_transform(self.y)\n",
        "            kf = StratifiedKFold(n_splits=self.num_folds)\n",
        "            train_prediction_shape = (lentrain, self.num_classes)\n",
        "        else:\n",
        "            self.num_classes = -1\n",
        "            self.y_enc = self.y\n",
        "            kf = KFold(n_splits=self.num_folds)\n",
        "            train_prediction_shape = (lentrain, 1)\n",
        "\n",
        "        self.train_prediction_dict = {}\n",
        "        for level in range(self.levels):\n",
        "            self.train_prediction_dict[level] = np.zeros((train_prediction_shape[0],\n",
        "                                                          train_prediction_shape[1] * len(self.model_dict[level])))\n",
        "\n",
        "        for level in range(self.levels):\n",
        "\n",
        "            if level == 0:\n",
        "                temp_train = self.training_data\n",
        "            else:\n",
        "                temp_train = self.train_prediction_dict[level - 1]\n",
        "\n",
        "            for model_num, model in enumerate(self.model_dict[level]):\n",
        "                validation_scores = []\n",
        "                foldnum = 1\n",
        "                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n",
        "                    logger.info(\"Training Level %d Fold # %d. Model # %d\", level, foldnum, model_num)\n",
        "\n",
        "                    if level != 0:\n",
        "                        l_training_data = temp_train[train_index]\n",
        "                        l_validation_data = temp_train[valid_index]\n",
        "                        model.fit(l_training_data, self.y_enc[train_index])\n",
        "                    else:\n",
        "                        l0_training_data = temp_train[0][model_num]\n",
        "                        if type(l0_training_data) == list:\n",
        "                            l_training_data = [x[train_index] for x in l0_training_data]\n",
        "                            l_validation_data = [x[valid_index] for x in l0_training_data]\n",
        "                        else:\n",
        "                            l_training_data = l0_training_data[train_index]\n",
        "                            l_validation_data = l0_training_data[valid_index]\n",
        "                        model.fit(l_training_data, self.y_enc[train_index])\n",
        "\n",
        "                    logger.info(\"Predicting Level %d. Fold # %d. Model # %d\", level, foldnum, model_num)\n",
        "\n",
        "                    if self.task_type == 'classification':\n",
        "                        temp_train_predictions = model.predict_proba(l_validation_data)\n",
        "                        self.train_prediction_dict[level][valid_index,\n",
        "                        (model_num * self.num_classes):(model_num * self.num_classes) +\n",
        "                                                       self.num_classes] = temp_train_predictions\n",
        "\n",
        "                    else:\n",
        "                        temp_train_predictions = model.predict(l_validation_data)\n",
        "                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n",
        "                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n",
        "                    validation_scores.append(validation_score)\n",
        "                    logger.info(\"Level %d. Fold # %d. Model # %d. Validation Score = %f\", level, foldnum, model_num,\n",
        "                                validation_score)\n",
        "                    foldnum += 1\n",
        "                avg_score = np.mean(validation_scores)\n",
        "                std_score = np.std(validation_scores)\n",
        "                logger.info(\"Level %d. Model # %d. Mean Score = %f. Std Dev = %f\", level, model_num,\n",
        "                            avg_score, std_score)\n",
        "\n",
        "            logger.info(\"Saving predictions for level # %d\", level)\n",
        "            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n",
        "            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\" + str(level) + \".csv\"),\n",
        "                                        index=False, header=None)\n",
        "\n",
        "        return self.train_prediction_dict\n",
        "\n",
        "    def predict(self, test_data, lentest):\n",
        "        self.test_data = test_data\n",
        "        if self.task_type == 'classification':\n",
        "            test_prediction_shape = (lentest, self.num_classes)\n",
        "        else:\n",
        "            test_prediction_shape = (lentest, 1)\n",
        "\n",
        "        self.test_prediction_dict = {}\n",
        "        for level in range(self.levels):\n",
        "            self.test_prediction_dict[level] = np.zeros((test_prediction_shape[0],\n",
        "                                                         test_prediction_shape[1] * len(self.model_dict[level])))\n",
        "        self.test_data = test_data\n",
        "        for level in range(self.levels):\n",
        "            if level == 0:\n",
        "                temp_train = self.training_data\n",
        "                temp_test = self.test_data\n",
        "            else:\n",
        "                temp_train = self.train_prediction_dict[level - 1]\n",
        "                temp_test = self.test_prediction_dict[level - 1]\n",
        "\n",
        "            for model_num, model in enumerate(self.model_dict[level]):\n",
        "\n",
        "                logger.info(\"Training Fulldata Level %d. Model # %d\", level, model_num)\n",
        "                if level == 0:\n",
        "                    model.fit(temp_train[0][model_num], self.y_enc)\n",
        "                else:\n",
        "                    model.fit(temp_train, self.y_enc)\n",
        "\n",
        "                logger.info(\"Predicting Test Level %d. Model # %d\", level, model_num)\n",
        "\n",
        "                if self.task_type == 'classification':\n",
        "                    if level == 0:\n",
        "                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n",
        "                    else:\n",
        "                        temp_test_predictions = model.predict_proba(temp_test)\n",
        "                    self.test_prediction_dict[level][:, (model_num * self.num_classes): (model_num * self.num_classes) +\n",
        "                                                                                        self.num_classes] = temp_test_predictions\n",
        "\n",
        "                else:\n",
        "                    if level == 0:\n",
        "                        temp_test_predictions = model.predict(temp_test[0][model_num])\n",
        "                    else:\n",
        "                        temp_test_predictions = model.predict(temp_test)\n",
        "                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n",
        "\n",
        "            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n",
        "            test_predictions_df.to_csv(os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n",
        "                                       index=False, header=None)\n",
        "\n",
        "        return self.test_prediction_dict"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHcXcucxpgdp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "267cff3a-6e47-4a19-89ab-e97a6238b665"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# specify the data to be used for every level of ensembling:\n",
        "train_data_dict = {0: [X_train_tfv, X_train_ctv, X_train_tfv, X_train_ctv], 1: [xtrain_glove]}\n",
        "test_data_dict = {0: [X_valid_tfv, X_valid_ctv, X_valid_tfv, X_valid_ctv], 1: [xvalid_glove]}\n",
        "\n",
        "model_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n",
        "\n",
        "              1: [xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}\n",
        "\n",
        "ens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n",
        "                optimize=multiclass_logloss, lower_is_better=True, save_path='')\n",
        "\n",
        "ens.fit(train_data_dict, y_train_enc, lentrain=xtrain_glove.shape[0])\n",
        "preds = ens.predict(test_data_dict, lentest=xvalid_glove.shape[0])"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10:31:55] INFO Found 7 classes\n",
            "[10:31:55] INFO Training Level 0 Fold # 1. Model # 0\n",
            "[10:31:57] INFO Predicting Level 0. Fold # 1. Model # 0\n",
            "[10:31:57] INFO Level 0. Fold # 1. Model # 0. Validation Score = 1.425598\n",
            "[10:31:57] INFO Training Level 0 Fold # 2. Model # 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:31:58] INFO Predicting Level 0. Fold # 2. Model # 0\n",
            "[10:31:58] INFO Level 0. Fold # 2. Model # 0. Validation Score = 1.402926\n",
            "[10:31:58] INFO Training Level 0 Fold # 3. Model # 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:31:59] INFO Predicting Level 0. Fold # 3. Model # 0\n",
            "[10:31:59] INFO Level 0. Fold # 3. Model # 0. Validation Score = 1.408808\n",
            "[10:31:59] INFO Level 0. Model # 0. Mean Score = 1.412444. Std Dev = 0.009607\n",
            "[10:31:59] INFO Training Level 0 Fold # 1. Model # 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:32:07] INFO Predicting Level 0. Fold # 1. Model # 1\n",
            "[10:32:07] INFO Level 0. Fold # 1. Model # 1. Validation Score = 1.475822\n",
            "[10:32:07] INFO Training Level 0 Fold # 2. Model # 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:32:14] INFO Predicting Level 0. Fold # 2. Model # 1\n",
            "[10:32:15] INFO Level 0. Fold # 2. Model # 1. Validation Score = 1.444641\n",
            "[10:32:15] INFO Training Level 0 Fold # 3. Model # 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:32:21] INFO Predicting Level 0. Fold # 3. Model # 1\n",
            "[10:32:22] INFO Level 0. Fold # 3. Model # 1. Validation Score = 1.463754\n",
            "[10:32:22] INFO Level 0. Model # 1. Mean Score = 1.461406. Std Dev = 0.012837\n",
            "[10:32:22] INFO Training Level 0 Fold # 1. Model # 2\n",
            "[10:32:22] INFO Predicting Level 0. Fold # 1. Model # 2\n",
            "[10:32:22] INFO Level 0. Fold # 1. Model # 2. Validation Score = 1.547975\n",
            "[10:32:22] INFO Training Level 0 Fold # 2. Model # 2\n",
            "[10:32:22] INFO Predicting Level 0. Fold # 2. Model # 2\n",
            "[10:32:22] INFO Level 0. Fold # 2. Model # 2. Validation Score = 1.500929\n",
            "[10:32:22] INFO Training Level 0 Fold # 3. Model # 2\n",
            "[10:32:22] INFO Predicting Level 0. Fold # 3. Model # 2\n",
            "[10:32:22] INFO Level 0. Fold # 3. Model # 2. Validation Score = 1.521596\n",
            "[10:32:22] INFO Level 0. Model # 2. Mean Score = 1.523500. Std Dev = 0.019254\n",
            "[10:32:22] INFO Training Level 0 Fold # 1. Model # 3\n",
            "[10:32:22] INFO Predicting Level 0. Fold # 1. Model # 3\n",
            "[10:32:22] INFO Level 0. Fold # 1. Model # 3. Validation Score = 2.029555\n",
            "[10:32:22] INFO Training Level 0 Fold # 2. Model # 3\n",
            "[10:32:22] INFO Predicting Level 0. Fold # 2. Model # 3\n",
            "[10:32:22] INFO Level 0. Fold # 2. Model # 3. Validation Score = 1.981980\n",
            "[10:32:22] INFO Training Level 0 Fold # 3. Model # 3\n",
            "[10:32:22] INFO Predicting Level 0. Fold # 3. Model # 3\n",
            "[10:32:22] INFO Level 0. Fold # 3. Model # 3. Validation Score = 1.957865\n",
            "[10:32:22] INFO Level 0. Model # 3. Mean Score = 1.989800. Std Dev = 0.029785\n",
            "[10:32:22] INFO Saving predictions for level # 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:32:22] INFO Training Level 1 Fold # 1. Model # 0\n",
            "[10:32:46] INFO Predicting Level 1. Fold # 1. Model # 0\n",
            "[10:32:46] INFO Level 1. Fold # 1. Model # 0. Validation Score = 1.496972\n",
            "[10:32:46] INFO Training Level 1 Fold # 2. Model # 0\n",
            "[10:33:10] INFO Predicting Level 1. Fold # 2. Model # 0\n",
            "[10:33:10] INFO Level 1. Fold # 2. Model # 0. Validation Score = 1.462039\n",
            "[10:33:10] INFO Training Level 1 Fold # 3. Model # 0\n",
            "[10:33:35] INFO Predicting Level 1. Fold # 3. Model # 0\n",
            "[10:33:35] INFO Level 1. Fold # 3. Model # 0. Validation Score = 1.476698\n",
            "[10:33:35] INFO Level 1. Model # 0. Mean Score = 1.478570. Std Dev = 0.014322\n",
            "[10:33:35] INFO Saving predictions for level # 1\n",
            "[10:33:35] INFO Training Fulldata Level 0. Model # 0\n",
            "[10:33:37] INFO Predicting Test Level 0. Model # 0\n",
            "[10:33:37] INFO Training Fulldata Level 0. Model # 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:33:44] INFO Predicting Test Level 0. Model # 1\n",
            "[10:33:44] INFO Training Fulldata Level 0. Model # 2\n",
            "[10:33:44] INFO Predicting Test Level 0. Model # 2\n",
            "[10:33:44] INFO Training Fulldata Level 0. Model # 3\n",
            "[10:33:44] INFO Predicting Test Level 0. Model # 3\n",
            "[10:33:45] INFO Training Fulldata Level 1. Model # 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:34:21] INFO Predicting Test Level 1. Model # 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2_JPyBCvSTm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8eb20387-04aa-4abe-807f-df1c98773900"
      },
      "source": [
        "# check error:\n",
        "multiclass_logloss(y_cv_enc, preds[1])"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5316193489752536"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJYPM1Jzw1I4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8b0c5961-060a-42fe-a647-709a8f15add8"
      },
      "source": [
        "# Also check train error\n",
        "preds = ens.predict(train_data_dict, lentest=xtrain_glove.shape[0])\n",
        "multiclass_logloss(y_train_enc, preds[1])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10:52:30] INFO Training Fulldata Level 0. Model # 0\n",
            "[10:52:31] INFO Predicting Test Level 0. Model # 0\n",
            "[10:52:31] INFO Training Fulldata Level 0. Model # 1\n",
            "[10:52:39] INFO Predicting Test Level 0. Model # 1\n",
            "[10:52:39] INFO Training Fulldata Level 0. Model # 2\n",
            "[10:52:39] INFO Predicting Test Level 0. Model # 2\n",
            "[10:52:39] INFO Training Fulldata Level 0. Model # 3\n",
            "[10:52:39] INFO Predicting Test Level 0. Model # 3\n",
            "[10:52:40] INFO Training Fulldata Level 1. Model # 0\n",
            "[10:53:14] INFO Predicting Test Level 1. Model # 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0485667689038733"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960NWrHG4DJg",
        "colab_type": "text"
      },
      "source": [
        "# INFERENCE MODELLING:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7KmiMuK0GV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.externals import joblib \n",
        "  \n",
        "# Save the model as a pickle in a file \n",
        "joblib.dump(ens, 'custom_ensembler.pkl') \n",
        "joblib.dump(model, 'lstm.pkl') \n",
        "joblib.dump(lr, 'lr2.pkl') \n",
        "joblib.dump(tfv, 'tfidf.pkl') "
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYuq58QCAXJJ",
        "colab_type": "text"
      },
      "source": [
        "### Utility Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FUj4uHw4yes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "### Dataset Preprocessing \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "def preprocessor(sentence):\n",
        "  ps = PorterStemmer()\n",
        "\n",
        "  review = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "\n",
        "  review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "  review = ' '.join(review)\n",
        "  return review"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbG8axNA_YMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mapper(ans):\n",
        "  for i in ans:\n",
        "    if ans==0:\n",
        "      return 'Anger'\n",
        "    elif ans==1:\n",
        "      return 'Disgust'\n",
        "    elif ans==2:\n",
        "      return 'Fear'\n",
        "    elif ans==3:\n",
        "      return 'Joy'\n",
        "    elif ans==4:\n",
        "      return 'Neutral'\n",
        "    elif ans==5:\n",
        "      return 'Sadness'\n",
        "    elif ans==6:\n",
        "      return 'Surprise'\n",
        "    \n",
        "    "
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfMd0s8O4OIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.externals import joblib \n",
        "def predictor(sentence):\n",
        "  lst = []\n",
        "  my_model = joblib.load('lr2.pkl') \n",
        "  tfv = joblib.load('tfidf.pkl')  \n",
        "\n",
        "  sent = preprocessor(sentence)\n",
        "\n",
        "  lst.append(sent)\n",
        "  sent_tfv = tfv.transform(lst)\n",
        "  ans = my_model.predict(sent_tfv) \n",
        "  mapped_ans = mapper(ans)\n",
        "  return mapped_ans"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyl6JfDqAcxX",
        "colab_type": "text"
      },
      "source": [
        "## Let's Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0i5N1oN6RWC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae121d05-b1be-4491-e26a-521258c821ae"
      },
      "source": [
        "ans = predictor('I hate you from bottom of my heart')\n",
        "print(ans)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anger\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv3hM02vAfZg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e496f88f-814b-4414-a358-a29e1f86b52a"
      },
      "source": [
        "ans = predictor('Oh Wow what a beautiful place')\n",
        "print(ans)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Joy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhrgZ9-AAlcK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1136c551-9656-4be1-fd37-c14b1bec437d"
      },
      "source": [
        "ans = predictor('I am really sorry , i am feeling reaaly very sad')\n",
        "print(ans)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sadness\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVym6Du_xe0T",
        "colab_type": "text"
      },
      "source": [
        "# CONCLUSION:\n",
        "1. The results of Infernece model are okay'ish since we used Logistic Regression + TFIDF which performed awesome.\n",
        "1. We see that ensembling improves the score by a great extent! \n",
        "2. LSTM can win this with higher number of epochs or altering the learnng rate. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LawXflvW37X6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}